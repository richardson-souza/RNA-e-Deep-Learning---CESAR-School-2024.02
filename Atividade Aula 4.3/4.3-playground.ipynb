{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02caaa80",
   "metadata": {},
   "source": [
    "## üß™ Experimento com TensorFlow Playground - Dataset \"Spiral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f52f8",
   "metadata": {},
   "source": [
    "![Texto alternativo](captura.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2ed20",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Configura√ß√£o do modelo\n",
    "- **Features usadas**: `x1`, `x2`\n",
    "- **Dataset**: Spiral\n",
    "- **N√∫mero de camadas ocultas**: 3\n",
    "- **Neur√¥nios por camada**: 8\n",
    "- **Fun√ß√£o de ativa√ß√£o**: `Tanh`\n",
    "- **Learning rate**: `0.03`\n",
    "- **Regulariza√ß√£o**: `L2`\n",
    "- **Taxa de regulariza√ß√£o**: `0.001`\n",
    "- **Noise**: 0\n",
    "- **Test loss**: `0.076`\n",
    "- **Training loss**: `0.010`\n",
    "\n",
    "---\n",
    "\n",
    "### üìå O que aprendi com esse experimento\n",
    "\n",
    "#### üîπ N√∫mero de camadas e neur√¥nios\n",
    "- Utilizar 3 camadas com 8 neur√¥nios cada permitiu que o modelo tivesse **capacidade suficiente para capturar a complexidade do padr√£o espiral**.\n",
    "- Quando usei menos camadas ou menos neur√¥nios, o modelo **subajustou** (n√£o conseguiu aprender a estrutura do dado).\n",
    "- Um n√∫mero excessivo de neur√¥nios sem regulariza√ß√£o pode levar ao **overfitting**, mas com controle adequado, melhora o desempenho.\n",
    "\n",
    "#### üîπ Fun√ß√£o de ativa√ß√£o\n",
    "- A fun√ß√£o `Tanh` se mostrou mais eficaz para esse dataset do que `ReLU` ou `Sigmoid`, pois ela **modela curvas suaves e sim√©tricas**, que s√£o necess√°rias para separar corretamente as espirais entrela√ßadas.\n",
    "\n",
    "#### üîπ Learning rate\n",
    "- Uma taxa de aprendizado de `0.03` ofereceu **converg√™ncia est√°vel e relativamente r√°pida**.\n",
    "- Taxas mais altas causaram instabilidade; taxas muito baixas tornaram o aprendizado muito lento.\n",
    "\n",
    "#### üîπ Regulariza√ß√£o\n",
    "- A regulariza√ß√£o `L2` com fator `0.001` foi essencial para **controlar a complexidade do modelo**, reduzindo overfitting e melhorando a generaliza√ß√£o (boa diferen√ßa entre Training e Test Loss).\n",
    "- Sem regulariza√ß√£o, o modelo tendia a **memorizar o conjunto de treino**, piorando o desempenho em dados de teste.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclus√£o\n",
    "\n",
    "Este exerc√≠cio demonstrou na pr√°tica como o equil√≠brio entre **profundidade da rede**, **capacidade expressiva**, **fun√ß√£o de ativa√ß√£o adequada**, e **regulariza√ß√£o** √© crucial para resolver problemas com fronteiras de decis√£o complexas, como o padr√£o espiral. Pequenas varia√ß√µes nesses hiperpar√¢metros causam impactos significativos na **capacidade de generaliza√ß√£o** do modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8a1a6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
